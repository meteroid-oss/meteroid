x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"
    tag: "{{.Name}}"

volumes:
  pg_data:
  redpanda_data:
  clickhouse_data:
  minio_data:

networks:
  meteroid_net:

name: meteroid-dev

services:

  ### Meteroid Dependencies
  meteroid-db:
    image: postgres:15.2
    container_name: meteroid-db
    ports:
      - '5432:5432'
    user: postgres
    environment:
      - POSTGRES_USER=${DATABASE_USER:-meteroid}
      - POSTGRES_PASSWORD=${DATABASE_PASSWORD}
      - POSTGRES_DB=${DATABASE_NAME:-meteroid}
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./init-db:/docker-entrypoint-initdb.d
    healthcheck:
      test: [ 'CMD-SHELL', 'pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}' ]
      interval: 10s
      timeout: 5s
      retries: 5
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
    networks:
      - meteroid_net

  svix-server:
    image: "docker.io/svix/svix-server:v1.61.0"
    environment:
      SVIX_JWT_SECRET: ${SVIX_JWT_SECRET:-changeMeSecret}
      SVIX_DB_DSN: postgresql://${DATABASE_USER:-meteroid}:${DATABASE_PASSWORD:-meteroid}@pgbouncer/svix
      SVIX_QUEUE_TYPE: "memory"
    ports:
      - "8071:8071"
    depends_on:
      - pgbouncer
    networks:
      - meteroid_net

  pgbouncer:
    image: "docker.io/edoburu/pgbouncer:v1.23.1-p2"
    healthcheck:
      test: "pg_isready -h localhost"
      interval: 30s
      timeout: 10s
      retries: 3
    environment:
      DB_HOST: "meteroid-db"
      DB_USER: "meteroid"
      DB_PASSWORD: ${DATABASE_PASSWORD}
      MAX_CLIENT_CONN: 200
      AUTH_TYPE: "scram-sha-256"
    depends_on:
      - meteroid-db
    networks:
      - meteroid_net

  gotenberg:
    image: gotenberg/gotenberg:8
    container_name: gotenberg
    ports:
      - 8073:3000
    networks:
      - meteroid_net

  minio:
    image: minio/minio:latest
    environment:
      - MINIO_ROOT_USER=${S3_USER:-meteroid}
      - MINIO_ROOT_PASSWORD=${S3_password:-meteroid}
    command: server /data --console-address ":9001"
    ports:
      - 9002:9000
      - 9001:9001
    volumes:
      - minio_data:/data
    networks:
      - meteroid_net
    healthcheck:
      test: [ "CMD", "mc", "ready", "local" ]
      interval: 3s
      timeout: 5s
      retries: 5

  createbuckets:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 ${S3_USER:-meteroid} ${S3_PASSWORD:-meteroid};
      /usr/bin/mc mb myminio/meteroid;
      exit 0;
      "
    networks:
      - meteroid_net

  debezium:
    image: quay.io/debezium/connect:2.7.3.Final
    depends_on:
      - redpanda
    ports:
      - 8083:8083
    environment:
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=connect_configs
      - OFFSET_STORAGE_TOPIC=connect_offsets
      - STATUS_STORAGE_TOPIC=connect_statuses
      - BOOTSTRAP_SERVERS=redpanda:29092
      # CONNECT_ properties are for the Connect worker
      #  - CONNECT_TOPIC_CREATION_ENABLE=true
      #  - CONNECT_TOPIC_CREATION_DEFAULT_REPLICATION_FACTOR=3
      #  - CONNECT_TOPIC_CREATION_DEFAULT_PARTITIONS=3
      - CONNECT_BOOTSTRAP_SERVERS=redpanda:29092
      #     - CONNECT_SECURITY_PROTOCOL=${KAFKA_SECURITY_PROTOCOL}
      #     - CONNECT_SASL_JAAS_CONFIG=${KAFKA_SASL_JAAS_CONFIG}
      #     - CONNECT_SASL_MECHANISM=${KAFKA_SASL_MECHANISM}
      #     - CONNECT_PRODUCER_SECURITY_PROTOCOL=${KAFKA_SECURITY_PROTOCOL}
      #     - CONNECT_PRODUCER_SASL_JAAS_CONFIG=${KAFKA_SASL_JAAS_CONFIG}
      #     - CONNECT_PRODUCER_SASL_MECHANISM=${KAFKA_SASL_MECHANISM}
      #     - CONNECT_CONSUMER_SECURITY_PROTOCOL=${KAFKA_SECURITY_PROTOCOL}
      #     - CONNECT_CONSUMER_SASL_JAAS_CONFIG=${KAFKA_SASL_JAAS_CONFIG}
      #     - CONNECT_CONSUMER_SASL_MECHANISM=${KAFKA_SASL_MECHANISM}
      - CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false
    networks:
      - meteroid_net

  ### Metering Dependencies
  clickhouse:
    image: clickhouse/clickhouse-server:23.12.1-alpine
    ports:
      - 8123:8123
      - 9000:9000
      - 9009:9009
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: default
      CLICKHOUSE_DB: meteroid
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://clickhouse:8123/ping || exit 1
      interval: 5s
      timeout: 3s
      retries: 100
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./volume/clickhouse/config.xml:/develop/clickhouse/config.xml
    networks:
      - meteroid_net
    profiles:
      - metering

  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:v23.3.1
    container_name: redpanda
    hostname: redpanda
    command:
      - redpanda start
      - --smp 1
      - --memory 1G
      - --overprovisioned
      - --node-id 0
      - --kafka-addr INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      - --advertise-kafka-addr INTERNAL://redpanda:29092,EXTERNAL://localhost:9092
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    networks:
      - meteroid_net
    ports:
      - 9092:9092

  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:v2.3.1
    entrypoint: /bin/sh
    command: -c "echo \"$$CONSOLE_CONFIG_FILE\" > /tmp/config.yml; /app/console"
    ports:
      - 8085:8080
      - 9644:9644
    networks:
      - meteroid_net
    environment:
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        kafka:
          brokers: ["redpanda:29092"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://redpanda:9644"]
    depends_on:
      - redpanda
    profiles:
      - metering

  redpanda-topic-create:
    image: docker.redpanda.com/redpandadata/redpanda:v23.3.1
    depends_on:
      - redpanda
    entrypoint: [ "bash", "-c", "sleep 5 && rpk topic create meteroid-events-raw --brokers redpanda:29092" ]
    networks:
      - meteroid_net
    profiles:
      - metering

  # ********************
  # Telemetry Components
  #   inspired by https://github.com/open-telemetry/opentelemetry-demo
  #   for development purposes only
  # ********************

  # Jaeger
  #   https://www.jaegertracing.io/docs/1.65/getting-started/#all-in-one
  jaeger:
    image: jaegertracing/all-in-one:1.65.0
    container_name: jaeger
    command:
      - "--memory.max-traces=25000"
      - "--query.base-path=/jaeger/ui"
      - "--prometheus.server-url=http://prometheus:9090"
      - "--prometheus.query.normalize-calls=true"
      - "--prometheus.query.normalize-duration=true"
    networks:
      - meteroid_net
    ports:
      # - '5778:5778'                   # serve configs (sampling, etc.)
      # closing ports because of using otelcol
      # - '4318:4318'                     # OTLP over HTTP
      - '4317'                          # OTLP over gRPC
      - '16686:16686'                   # jaeger frontend
    environment:
      - METRICS_STORAGE_TYPE=prometheus # https://www.jaegertracing.io/docs/1.65/deployment/#metrics-storage-backends
    deploy:
      resources:
        limits:
          memory: 300M
    restart: unless-stopped
    profiles:
      - telemetry
    logging: *logging

  # Grafana
  grafana:
    image: grafana/grafana:11.4.0
    container_name: grafana
    networks:
      - meteroid_net
    ports:
      - "3000:3000"
    environment:
      - GF_SERVER_ROOT_URL=http://grafana:3000
      - "GF_INSTALL_PLUGINS=grafana-opensearch-datasource"
    volumes:
      - ./volume/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./volume/grafana/provisioning/:/etc/grafana/provisioning/
    deploy:
      resources:
        limits:
          memory: 120M
    profiles:
      - telemetry
    logging: *logging

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.118.0
    container_name: otel-collector
    command: [ "--config=/etc/otelcol-config.yml", "--config=/etc/otelcol-config-extras.yml" ]
    user: 0:0
    volumes:
      - /:/hostfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./volume/otelcollector/otelcol-config.yml:/etc/otelcol-config.yml
      - ./volume/otelcollector/otelcol-config-extras.yml:/etc/otelcol-config-extras.yml
    networks:
      - meteroid_net
    ports:
      # same as jaeger ports
      - "4317:4317"                        # OTLP over gRPC receiver
      - "4318:4318"                        # OTLP over HTTP receiver
    depends_on:
      jaeger:
        condition: service_started
      opensearch:
        condition: service_healthy
      loki:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 125M
    restart: unless-stopped
    profiles:
      - telemetry
    logging: *logging

  loki:
    image: grafana/loki:3.3.2
    container_name: loki
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./volume/loki/local-config.yaml:/etc/loki/local-config.yaml
    networks:
      - meteroid_net
    ports:
      - "3100:3100"
    deploy:
      resources:
        limits:
          memory: 100M
    profiles:
      - telemetry
    logging: *logging

  # Prometheus
  prometheus:
    image: quay.io/prometheus/prometheus:v3.1.0
    container_name: prometheus
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --web.enable-otlp-receiver
      - --enable-feature=exemplar-storage
    volumes:
      - ./volume/prometheus/prometheus-config.yaml:/etc/prometheus/prometheus-config.yaml
    networks:
      - meteroid_net
    ports:
      - "9090:9090"
    deploy:
      resources:
        limits:
          memory: 300M
    profiles:
      - telemetry
    logging: *logging

  opensearch:
    image: opensearchproject/opensearch:2.18.0
    container_name: opensearch
    environment:
      - cluster.name=demo-cluster
      - node.name=demo-node
      - bootstrap.memory_lock=true
      - discovery.type=single-node
      - OPENSEARCH_JAVA_OPTS=-Xms300m -Xmx300m
      - DISABLE_INSTALL_DEMO_CONFIG=true
      - DISABLE_SECURITY_PLUGIN=true
      # Workaround on OSX for https://bugs.openjdk.org/browse/JDK-8345296
      - _JAVA_OPTIONS
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    networks:
      - meteroid_net
    ports:
      - "9200:9200"
    healthcheck:
      test: curl -s http://localhost:9200/_cluster/health | grep -E '"status":"(green|yellow)"'
      start_period: 10s
      interval: 5s
      timeout: 10s
      retries: 10
    profiles:
      - telemetry
    logging: *logging
